---
---

----------------------------------------------------------------------------------------------------------------------------
--- 2025
----------------------------------------------------------------------------------------------------------------------------


@article{vigano2025nam,
    title = {NAM: Neural Adjoint Maps for refining shape correspondences},
    journal = {Transactions On Graphics},
    volume = {122},
    pages = {103985},
    year = {2025},
    selected = {true},
    abbr = {SIGGRAPH},
    issn = {0097-8493},
    doi = {https://doi.org/10.1145/3730943},
    url = {https://www.lix.polytechnique.fr/~maks/papers/NAM_SIGGRAPH2025.pdf},
    author = {Viganò, Giulio and Ovsjanikov, Maks and Melzi, Simone},
    keywords = {Shape correspondence, Functional maps, Point clouds, Machine
                learning},
    preview = {graphical_abstract_1494.jpg},
}

@misc{blend25,
    abbr = {Preprint},
    title = {Blending Concepts with Text-to-Image Diffusion Models},
    author = {Lorenzo Olearo and Giorgio Longari and Alessandro Raganato and
              Rafael Peñaloza and Simone Melzi},
    year = {2025},
    eprint = {2506.23630},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV},
    url = {https://arxiv.org/abs/2506.23630},
    abstract = { Diffusion models have dramatically advanced text-to-image
                generation in recent years, translating abstract concepts into
                high-fidelity images with remarkable ease. In this work, we
                examine whether they can also blend distinct concepts, ranging
                from concrete objects to intangible ideas, into coherent new
                visual entities under a zero-shot framework. Specifically,
                concept blending merges the key attributes of multiple concepts
                (expressed as textual prompts) into a single, novel image that
                captures the essence of each concept. We investigate four
                blending methods, each exploiting different aspects of the
                diffusion pipeline (e.g., prompt scheduling, embedding
                interpolation, or layer-wise conditioning). Through systematic
                experimentation across diverse concept categories, such as
                merging concrete concepts, synthesizing compound words,
                transferring artistic styles, and blending architectural
                landmarks, we show that modern diffusion models indeed exhibit
                creative blending capabilities without further training or
                fine-tuning. Our extensive user study, involving 100 participants
                , reveals that no single approach dominates in all scenarios:
                each blending technique excels under certain conditions, with
                factors like prompt ordering, conceptual distance, and random
                seed affecting the outcome. These findings highlight the
                remarkable compositional potential of diffusion models while
                exposing their sensitivity to seemingly minor input variations. },
    pdf = {https://arxiv.org/pdf/2506.23630},
    html = {https://arxiv.org/abs/2506.23630},
    preview = {preview-blending-concepts.png},
}

@inproceedings{Hadgi_2025_CVPR,
    author = {Hadgi, Souhail and Moschella, Luca and Santilli, Andrea and Gomez,
              Diego and Huang, Qixing and Rodol\`a, Emanuele and Melzi, Simone
              and Ovsjanikov, Maks},
    title = {Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent
             Spaces},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and
                 Pattern Recognition (CVPR)},
    month = {June},
    year = {2025},
    pages = {19825-19835},
    doi = {10.48550/arXiv.2503.05283},
    url = {
           https://openaccess.thecvf.com/content/CVPR2025/papers/Hadgi_Escaping_Platos_Cave_Towards_the_Alignment_of_3D_and_Text_CVPR_2025_paper.pdf
           },
}

----------------------------------------------------------------------------------------------------------------------------
--- 2024
----------------------------------------------------------------------------------------------------------------------------

@inproceedings{maccarone2024sfora,
    booktitle = {Smart Tools and Applications in Graphics - Eurographics Italian
                 Chapter Conference},
    editor = {Caputo, Ariel and Garro, Valeria and Giachetti, Andrea and
              Castellani, Umberto and Dulecha, Tinsae Gebrechristos},
    title = {{S4A: Scalable Spectral Statistical Shape Analysis}},
    author = {Maccarone, Francesca and Longari, Giorgio and Viganò, Giulio and
              Peruzzo, Denis and Maggioli, Filippo and Melzi, Simone},
    year = {2024},
    publisher = {The Eurographics Association},
    ISSN = {2617-4855},
    ISBN = {978-3-03868-265-3},
    DOI = {10.2312/stag.20241343},
}

@article{vigano2024bule,
    title = {Bijective upsampling and learned embedding for point clouds
             correspondences},
    journal = {Computers and Graphics},
    volume = {122},
    pages = {103985},
    year = {2024},
    issn = {0097-8493},
    doi = {https://doi.org/10.1016/j.cag.2024.103985},
    url = {https://www.sciencedirect.com/science/article/pii/S0097849324001201},
    author = {Viganò, Giulio and Melzi, Simone},
    keywords = {Shape correspondence, Functional maps, Point clouds, Machine
                learning},
}


@article{olearo2024blend,
    title = {How to Blend Concepts in Diffusion Models},
    author = {Lorenzo Olearo and Giorgio Longari and Simone Melzi and Alessandro
              Raganato and Rafael Peñaloza},
    journal = {arXiv preprint arXiv:2407.14280},
    abbr = {KGM @ ECCV},
    year = {2024},
    abstract = { For the last decade, there has been a push to use
                multi-dimensional (latent) spaces to represent concepts; and yet
                how to manipulate these concepts or reason with them remains
                largely unclear. Some recent methods exploit multiple latent
                representations and their connection, making this research
                question even more entangled. Our goal is to understand how
                operations in the latent space affect the underlying concepts. To
                that end, we explore the task of concept blending through
                diffusion models. Diffusion models are based on a connection
                between a latent representation of textual prompts and a latent
                space that enables image reconstruction and generation. This task
                allows us to try different text-based combination strategies, and
                evaluate easily through a visual analysis. Our conclusion is that
                concept blending through space manipulation is possible, although
                the best strategy depends on the context of the blend. },
    pdf = {https://arxiv.org/pdf/2407.14280},
    html = {https://arxiv.org/pdf/2407.14280},
    preview = {preview-how-to-blend-concepts.png},
}

@inproceedings{10.2312:stag.20241345,
    booktitle = {Smart Tools and Applications in Graphics - Eurographics Italian
                 Chapter Conference},
    editor = {Caputo, Ariel and Garro, Valeria and Giachetti, Andrea and
              Castellani, Umberto and Dulecha, Tinsae Gebrechristos},
    title = {{Localized Gaussians as Self-Attention Weights for Point Clouds
             Correspondence}},
    author = {Riva, Alessandro and Raganato, Alessandro and Melzi, Simone},
    year = {2024},
    publisher = {The Eurographics Association},
    abbr = {STAG},
    ISSN = {2617-4855},
    ISBN = {978-3-03868-265-3},
    DOI = {10.2312/stag.20241345},
    preview = {preview-riva-stag2024.png},
}
